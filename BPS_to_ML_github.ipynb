{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BPS_to_ML_github.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Kx7zuTArHW5w"},"source":["Upload input-output datasets from Monte Carlo simulations and perform sensitivity analysis and construct fast metamodels.\n","\n","[Link to repository in Github](https://github.com/TorbenOestergaard/mc_sa_ml)\n"]},{"cell_type":"markdown","metadata":{"id":"MNt1s_qFT3Z8"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"nHETwzrUHiEw"},"source":["This notebook relies on several scripts and includes an example data file with Monte Carlo simulation results. Therefore, the notebook cannot be run by itself and it must have access to these resources. Below, you'll see three options to provide this access. Select the one that fits you best."]},{"cell_type":"markdown","metadata":{"id":"WD3LtyzYtfJk"},"source":["## Option A: Public github notebook"]},{"cell_type":"markdown","metadata":{"id":"iFHv2chhPqdo"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TorbenOestergaard/mc_sa_ml/blob/main/BPS_to_ML_github.ipynb)\n","\n","The fastest way to get started is to work online in the public github notebook.  \n","The content in the github repository *mc_sa_ml* must be cloned to the current Colab session in order to access the scripts and the example file. \n","\n","- Requirements: No requirements\n","- Changes: Your changes will *not* be saved when closing the browser."]},{"cell_type":"code","metadata":{"id":"KaKR9jg3tfJk"},"source":["!git clone https://github.com/TorbenOestergaard/mc_sa_ml.git\n","\n","my_path = 'mc_sa_ml/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUfQWp0wUHUW"},"source":["## Option B: Google Drive"]},{"cell_type":"markdown","metadata":{"id":"TxDqyNUpOikV"},"source":["You may import the folder Github repository *mc_sa_ml* and copy it to your Google Drive folder, for example to the 'Colab Notebooks\" subfolder which is created when you make your first Colab notebook.  \n","\n","- Requirements: Google Colab must be allowed to access the content in your Drive folder.\n","- Changes: Your changes will automatically be saved in your local notebook and files will be saved to your Drive folder. "]},{"cell_type":"code","metadata":{"id":"Bx59WND9-TOD"},"source":["# from google.colab import drive \n","# drive.mount('/content/drive')\n","\n","# my_path = '/content/drive/MyDrive/Colab Notebooks/mc_sa_ml/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfnz4e6ty25i"},"source":["## Option C: Local Python interpreter"]},{"cell_type":"markdown","metadata":{"id":"uFGMtYneNtOu"},"source":["Finally, you may clone the *mc_sa_ml* repository to your local desktop and run it using your own Python interpreter. However, you need to install a number of packages (see imports below) and set the `my_path` variable accordingly."]},{"cell_type":"code","metadata":{"id":"Xq2v6k6YOMKA"},"source":["# my_path = 'your_local_path_to_mc_sa_ml_folder'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3x3iX7QULeM"},"source":["## Import dependencies"]},{"cell_type":"code","metadata":{"id":"eb8EuYML-x8R"},"source":["global y_train_pred\n","global y_train_full_pred\n","global y_valid_pred\n","global y_test_pred\n","\n","global x_train_prepared\n","global x_train_full_prepared\n","global x_valid_prepared\n","global x_test_prepared\n","\n","from os import path, makedirs\n","import sys\n","from google.colab import files\n","import io\n","\n","sys.path.append(my_path)\n","\n","import scripts as src\n","from scripts import transform_categorical_features, ordinal_decode_cat_hyperparameters, create_new_samples, print_R2_performance, make_predictions, sa_multiple, obtain_pdfs \n","import SAtom2 as sa\n","\n","# Data analysis and visualization\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import clear_output \n","from ipywidgets import Dropdown\n","from os import path, makedirs\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import r2_score\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPXTHMXy3gWG"},"source":["# Upload your data"]},{"cell_type":"markdown","metadata":{"id":"_nmr5m9O1Myx"},"source":["## Upload file"]},{"cell_type":"markdown","metadata":{"id":"Ik_Ar4iKTBrG"},"source":["Run cell below to create upload button. Then select a file with Monte Carlo simulations inputs and outputs."]},{"cell_type":"code","metadata":{"id":"Q2oqKygU1O3H"},"source":["uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vipqoyGtTTSQ"},"source":["Run to load the file, if present, transform categorical features to numeric to enable sensitivity analysis and machine learning."]},{"cell_type":"code","metadata":{"id":"CqMxgk7j_Q0k"},"source":["try:\n","  filename_monte_carlo = list(uploaded.keys())[0]\n","except:\n","  filename_monte_carlo = 'test_high_school_i10_o3.tsv'\n","print(f'Filename:   {filename_monte_carlo}')\n","\n","file_extension = filename_monte_carlo.split(\".\")[-1]\n","file_name_only = filename_monte_carlo[:-(len(file_extension)+1)]\n","\n","if file_extension == 'xlsx':\n","  # Excel file. If data in specific sheet, add argument: sheet_name='Sheet1'\n","  XY_raw = pd.read_excel(my_path + filename_monte_carlo, header=0, engine=\"openpyxl\", )\n","elif file_extension == 'csv' or file_extension == 'txt':\n","  XY_raw = pd.read_csv(my_path +  filename_monte_carlo, sep=None)\n","elif file_extension == 'tsv':\n","  XY_raw = pd.read_csv(my_path + filename_monte_carlo, sep='\\t')\n","print(f'N rows:     {XY_raw.shape[0]} \\nN columns:  {XY_raw.shape[1]}')\n","\n","# Transform pcategorical features\n","XY = transform_categorical_features(XY_raw, verbose=False)\n","\n","XY_raw.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4vIopqYzISY4"},"source":["## Describe columns (important)"]},{"cell_type":"markdown","metadata":{"id":"v8Qxw4OPUqzB"},"source":["Specify the number of inputs and outputs to enable the scripts to distinguish between input columns and output columns."]},{"cell_type":"code","metadata":{"id":"scFUZICa_sHT"},"source":["n_inputs = 10 # Specify the number of input columns\n","n_outputs = 3 # Specify the number of output columns\n","\n","N = XY.shape[0]\n","X = XY.iloc[:N, :n_inputs]\n","Y = XY.iloc[:N, -n_outputs:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAxk2eaMdwSl"},"source":["Optionally, save as txt-file which can be uploaded to e.g. [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/)."]},{"cell_type":"code","metadata":{"id":"d1g57b1Wdvtz"},"source":["print('Saving to: ' + my_path + filename_monte_carlo[:-4] + 'csv')\n","# XY.to_csv(my_path + filename_monte_carlo[:-4] + 'csv', sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJxo73p-Uv0A"},"source":["# Sensitivity Analysis"]},{"cell_type":"markdown","metadata":{"id":"j3xGiM19UC3r"},"source":["Sensitivity analysis is performed using the *TOM* method which estimates the inputs' *total effects* on the outputs. It means that the contribution from 'interaction effects' between e.g. input *X1* and *X2* will be included in the sensitivity measure for both inputs. Thus it can be used in a *factor fixing* context where the object is to identify and potentially fix non-influential inputs (see [Global Sensitivity Analysis: The Primer](http://www.andreasaltelli.eu/file/repository/A_Saltelli_Marco_Ratto_Terry_Andres_Francesca_Campolongo_Jessica_Cariboni_Debora_Gatelli_Michaela_Saisana_Stefano_Tarantola_Global_Sensitivity_Analysis_The_Primer_Wiley_Interscience_2008_.pdf) for sensitivity analysis theory).\n","\n","The *TOM* method may be applied to all outputs simulationeously as means to order the inputs by the overall influence on all outputs. This is makes it easier to observe changes of the overall most important inputs when exploring the data using [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/). It does not simply asign equal weights to all outputs but instead reduce weights to highly correlated outputs. \n","\n","A detailed description of the *TOM* method is available in this [conference paper](https://vbn.aau.dk/da/publications/interactive-building-design-space-exploration-using-regionalized-)."]},{"cell_type":"markdown","metadata":{"id":"fYGqL0z35F2g"},"source":["## Single output"]},{"cell_type":"markdown","metadata":{"id":"oWicxjc1UyPJ"},"source":["Use dropdown to specify which output should be addressed in the sensitivity analysis."]},{"cell_type":"code","metadata":{"id":"vG0TYSv_AGlS"},"source":["output_labels = list(Y.columns)\n","dropdown = Dropdown(options= ['All'] + list(output_labels), description='Output(s):')\n","dropdown"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ff6D0i5yVGiV"},"source":["Set sensitivy arguments and perform analysis.  \n","For large datasets, start with a small J value (e.g. 30) for a single output and see if the SA estimates have converged. If not, increase J until they converge. "]},{"cell_type":"code","metadata":{"id":"Su5uxB7RAM98"},"source":["J = 30              # Number random splits. Increase if values have not converged\n","add_dummy = True    # Specify if a 'dummy' variable should be added\n","\n","if dropdown.value == 'All':\n","  print('Performing sensitivity analysis for all outputs.')\n","  y = Y.copy()\n","else:\n","  print('Performing sensitivity analysis for: ' + dropdown.value)\n","  y = Y.iloc[:, output_labels.index(dropdown.value)]\n","\n","tom = sa.TOM(X, y, J=J, verbose=True, dummy=add_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00Zehmk69jbA"},"source":["`tom.KS_df` contains the J-averaged Kolmogorov-Smirnov distances and can be used to make convergence plots (in relation to J).  \n","In the `df_SA` DataFrame sensitivity measures are rescaled to sum to 100%. Both DataFrames can be used to make your own SA figures."]},{"cell_type":"code","metadata":{"id":"6qb6W2eO-YV1"},"source":["df_SA = pd.DataFrame(columns=['Input', 'SA, tom'])\n","SA_score = tom.KS_df.iloc[-1,:].values # The last J'th averaged sample is used\n","SA_score = np.array([(val / sum(SA_score)) * 100 for val in SA_score])\n","for i, (col, score) in enumerate(zip(tom.KS_df.columns, SA_score)):\n","  df_SA.loc[i] = [col, score]\n","df_SA.sort_values(by=\"SA, tom\", ascending=False) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CM6MzPd2GMcL"},"source":["Optionally, order input columns by their influence with respect to a given output (or all outputs). The most influential will then be placed closest to the output columns, which make analysis in [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/) easier. "]},{"cell_type":"code","metadata":{"id":"n63sTFDOGK1J"},"source":["filename_export = 'sorted_XY'  # Define filename\n","\n","XY_export = XY.copy()\n","sa_sorted_inputs = list(df_SA.drop(df_SA[df_SA['Input'] == 'Dummy'].index).sort_values(by=\"SA, tom\", ascending=True)['Input'])\n","sorted_columns = sa_sorted_inputs + list(output_labels)\n","XY_export = XY_export[sorted_columns]\n","\n","# Save as csv or xlsx\n","XY_export.to_csv(my_path + filename_export + '.csv', sep='\\t', index=False)\n","# XY_export.to_excel(my_path + filename_export + '.xlsx', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9M8qGXR5JZc"},"source":["## All outputs\n","Create a figure with histogram and SA bar plots for all outputs."]},{"cell_type":"code","metadata":{"id":"mK36Vs0RXYnE"},"source":["# Enable figures to be return from function\n","%config InlineBackend.close_figures = False\n","\n","N_samples = X.shape[0] # Optionally, reduce the number of samples for faster SA\n","res, fig = sa_multiple(X.iloc[:N_samples,:], \n","                       Y.iloc[:N_samples,:], \n","                       J=100, \n","                       include_SA_all=False, \n","                       sort_by='all',\n","                       figsize='auto', # (7,5)\n","                       )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZH9gfueo5gPf"},"source":["Optionally, save the figure as jpg, png, or svg."]},{"cell_type":"code","metadata":{"id":"l5LOoj6wIL1I"},"source":["SAVE_FIG = False\n","if SAVE_FIG:\n","  # fig.savefig(my_path + file_name_only + '_TOM_SA.svg')\n","  # fig.savefig(my_path + file_name_only + '_TOM_SA.png')  \n","  fig.savefig(my_path + file_name_only + '_TOM_SA.jpg', dpi=300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdKWqXTeLt2I"},"source":["## External resources"]},{"cell_type":"markdown","metadata":{"id":"veoP1tQNL00q"},"source":["- Free textbook on sensitivity analysis: [Global Sensitivity Analysis – The Primer](http://www.andreasaltelli.eu/file/repository/A_Saltelli_M_Ratto_T_Andres_F_Campolongo_J_Cariboni_D_Gatelli_M_Saisana_S_Tarantola_Global_Sensitivity_Analysis_The_Primer_Wiley_Interscience_2008_errata_corrige.pdf)\n","- [Conference paper](https://vbn.aau.dk/da/publications/interactive-building-design-space-exploration-using-regionalized-) on TOM and TOR methods\n","- [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/) or [DesignExplorer](http://tt-acm.github.io/DesignExplorer/) for interactive design space exploration\n","- [SIMLAB & other software](https://ec.europa.eu/jrc/en/samo/simlab) from EU Science Hub\n","- UQlab & other software from [UQworld](https://uqworld.org/)\n","- [Online calculator](https://www.sobolindices.com/) of Sobol indices for datasets of max. 10 inputs and 500 samples (see [documentation](https://www.sobolindices.com/howto/))"]},{"cell_type":"markdown","metadata":{"id":"nxRRaAeqVdrg"},"source":["# Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"bACHMOUaNa8j"},"source":["## Choose output"]},{"cell_type":"markdown","metadata":{"id":"dzsp7pLRYzSw"},"source":["Run below cell to make a dropdown to choose which output should be used in the supervised machine learning. Alternatively, all outputs can be included but accuracy will be lower for the individual outputs."]},{"cell_type":"code","metadata":{"id":"2FMUQUMzNC7q"},"source":["output_labels = list(Y.columns)\n","dropdown_ml = Dropdown(options=['All (NN only)'] + list(output_labels), description='Output:')\n","dropdown_ml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52iMYedAVf0i"},"source":["## Prepare data"]},{"cell_type":"markdown","metadata":{"id":"G66wp8kEVmV6"},"source":["Split dataset into training set, validation set, and test set."]},{"cell_type":"code","metadata":{"id":"qoBUUpynKl0v"},"source":["VALID_SIZE = 0.25\n","TEST_SIZE = 0.25\n","shuffle_state = True\n","np.random.seed(42)\n","\n","# Create a test set and a \"full\" training set to be split into training and validation sets\n","train_set_full, test_set = train_test_split(XY, test_size=TEST_SIZE, shuffle=shuffle_state)\n","train_set, valid_set = train_test_split(train_set_full, test_size=(VALID_SIZE / (1 - TEST_SIZE)), shuffle=shuffle_state)\n","\n","total_sim = train_set.shape[0] + valid_set.shape[0] + test_set.shape[0]\n","display(f'Total of {total_sim} simulations. Length of data_raw: {len(XY)}.')\n","display(f'train set {train_set.shape}, valid set{valid_set.shape}, test set {test_set.shape}.')\n","display(f'Split ratios: {round((train_set.shape[0]/total_sim*100))} / {round((valid_set.shape[0]/total_sim*100))} / {round((test_set.shape[0]/total_sim*100))} %')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42qLfXukVwFh"},"source":["Split each set into input and output subsets and transform data for machine learning."]},{"cell_type":"code","metadata":{"id":"l-SxqgadL0iK"},"source":["x_train_full = train_set_full.iloc[:, :n_inputs]\n","x_train = train_set.iloc[:, :n_inputs]\n","x_valid = valid_set.iloc[:, :n_inputs]\n","x_test = test_set.iloc[:, :n_inputs]\n","\n","if dropdown_ml.value[:3] == 'All':\n","  y_train_full = train_set_full.iloc[:, n_inputs:]\n","  y_train = train_set.iloc[:, n_inputs:]\n","  y_valid = valid_set.iloc[:, n_inputs:]\n","  y_test = test_set.iloc[:, n_inputs:]\n","  y_labels = train_set.iloc[:, n_inputs:].columns.to_list()\n","else:\n","  y_train_full = train_set_full[dropdown_ml.value].copy()\n","  y_train = train_set[dropdown_ml.value].copy()\n","  y_valid = valid_set[dropdown_ml.value].copy()\n","  y_test = test_set[dropdown_ml.value].copy()\n","  y_labels = [dropdown_ml.value]\n","\n","num_pipeline = Pipeline([('standard_scaler', StandardScaler()),])\n","numerical_attr = x_train.select_dtypes(include=['float64', 'int64']).columns\n","\n","full_pipeline = ColumnTransformer([\n","    (\"num\", num_pipeline, numerical_attr), \n","    # (\"ord\", ordinal_pipeline, ord_attr),\n","    # (\"cat\", cat_pipeline, categorical_attr),\n","])\n","\n","x_train_prepared = full_pipeline.fit_transform(x_train)\n","x_train_full_prepared = full_pipeline.fit_transform(x_train_full)\n","x_valid_prepared = full_pipeline.transform(x_valid)\n","x_test_prepared = full_pipeline.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2B3Z5E3aV3Lm"},"source":["## Train models"]},{"cell_type":"markdown","metadata":{"id":"qgN0QizrWWun"},"source":["### Neural network"]},{"cell_type":"markdown","metadata":{"id":"lSTLgvyzJREA"},"source":["#### Single model"]},{"cell_type":"markdown","metadata":{"id":"9NA-MMEWEBNm"},"source":["Train a neural network *without* hyperparameter optimization. "]},{"cell_type":"code","metadata":{"id":"0IvS-BCdTomA"},"source":["# Define model parameters\n","model = MLPRegressor(hidden_layer_sizes=(20, 20), \n","                  learning_rate_init=0.05, \n","                  alpha=0.001, \n","                  early_stopping=True, \n","                  max_iter=500,  )\n","\n","# Train neural network\n","model.fit(x_train_full_prepared, y_train_full.values)\n","\n","# Make predictions\n","# (y_train_pred, y_train_full_pred, y_valid_pred, y_test_pred) = make_predictions(model);\n","y_train_pred = model.predict(x_train_prepared)\n","y_train_full_pred = model.predict(x_train_full_prepared)\n","y_valid_pred = model.predict(x_valid_prepared)\n","y_test_pred = model.predict(x_test_prepared)\n","\n","# Clear output cell and show only R² values (averaged if multiple outputs)\n","clear_output() \n","print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSQWCyuaEQyy"},"source":["See R²-values for each output."]},{"cell_type":"code","metadata":{"id":"V4HVq7rUCZTJ"},"source":["print(f'{\" \" * 18}TRAIN / TEST')\n","if y_train.ndim == 1:\n","  # print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f} -> Output: \"{y_train.to_frame().columns[0]}\"')\n","  print(f'{y_train.to_frame().columns[0]:>15}   {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}')\n","else:\n","  for i in range(y_train_pred.shape[1]):\n","    weights = np.zeros(y_train_pred.shape[1])\n","    weights[i] = 1\n","    r2_train_i = r2_score(y_train_full, y_train_full_pred, multioutput=weights)\n","    r2_test_i = r2_score(y_test, y_test_pred, multioutput=weights)\n","    print(f'{y_train.columns[i]:>15}   {r2_train_i:.3f} / {r2_test_i:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFTOMFWFJbQW"},"source":["#### Grid-search optimization"]},{"cell_type":"markdown","metadata":{"id":"5GLdkCgaEVqW"},"source":["Perform grid search to optimize hyperparameters. For more hyperparameters see [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).  \n","The number trained models equals the product of options for each hyperparameter multiplied by the number of cross validations (cv)."]},{"cell_type":"code","metadata":{"id":"eHyqvHwYblJt"},"source":["cross_validations = 3\n","\n","# Define hyperparameters to run grid search over\n","param_grid = [\n","              {'hidden_layer_sizes':[(20, 20), (50, 50), (20, 20, 20)], \n","               'learning_rate_init':[0.001, 0.01, 0.03], \n","              #  'learning_rate': ['constant', 'invscaling', 'adaptive'],\n","               'alpha':[0.001, 0.01], # default 0.001\n","               }\n","              ]\n","\n","# Setup fixed hyperparameters\n","model = MLPRegressor(#alpha=0.001, # default 0.0001, \n","                  early_stopping=True, max_iter=500, )\n","\n","# Set grid search options\n","grid_search = GridSearchCV(model, param_grid, cv=cross_validations,\n","                          scoring='neg_mean_squared_error',\n","                          return_train_score=True, verbose=1, n_jobs=-1)\n","\n","# Run grid search\n","grid_search.fit(x_train_full_prepared, y_train_full.values)\n","\n","clear_output() # Clear output cell\n","\n","# See cross validation results (RMSE) from grid search\n","cvres = grid_search.cv_results_ # Cross validation results from grid search\n","params_count = len(cvres['params'][0].keys()) # Number of hyperparameters\n","params_labels = list(cvres['params'][0].keys()) # Labels for hyperparameters\n","cvres_df = pd.DataFrame(columns=['RMSE'] + list(cvres['params'][0].keys()))\n","for i in range(len(cvres['mean_test_score'])):\n","  cvres_df = cvres_df.append({'RMSE': np.sqrt(-cvres['mean_test_score'][i]), **cvres['params'][i]}, ignore_index=True, )\n","\n","# Performance of best model\n","model = grid_search.best_estimator_\n","# (y_train_pred, y_train_full_pred, y_valid_pred, y_test_pred) = make_predictions(model);\n","y_train_pred = model.predict(x_train_prepared)\n","y_train_full_pred = model.predict(x_train_full_prepared)\n","y_valid_pred = model.predict(x_valid_prepared)\n","y_test_pred = model.predict(x_test_prepared)\n","print(f'Best model:  {grid_search.best_params_} \\nLowest RMSE: {cvres_df.RMSE.min():.3f}')\n","print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}\\n')\n","\n","display(cvres_df.sort_values(by='RMSE'))\n","cvres_df = ordinal_decode_cat_hyperparameters(cvres_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGSUfO4ROnl3"},"source":["Perform SA on hyperparameters to see which have most influence on performance. This indicates which hyperparameters that require most attention and could be discretized further in additional grid searches. May not work if only two parameters have been varied with two discretizations."]},{"cell_type":"code","metadata":{"id":"PNNy6YiSr3fp"},"source":["sa.TOM(cvres_df.drop('RMSE', axis=1), cvres_df.RMSE, J=100, dummy=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"elJSZ17VOvgL"},"source":["See hyperparameter distributions leading to the 20% best RMSE.  \n","This may reveal suitable values for the most important hyperparameters (among the considered parameters)."]},{"cell_type":"code","metadata":{"id":"4ZegBcNcODd0"},"source":["cvres_df.loc[cvres_df.RMSE > cvres_df.RMSE.quantile(0.8)].drop('RMSE', axis=1).hist(\n","    bins=100, figsize=(cvres_df.shape[1]*3,2.5), layout=(1, cvres_df.shape[1]-1), grid=False);\n","cvres_df.loc[cvres_df.RMSE > cvres_df.RMSE.quantile(0.8)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1nmRRmfWQIA"},"source":["### Plot y-y scatter"]},{"cell_type":"markdown","metadata":{"id":"jh8z2Bfla_bw"},"source":["Create a plot of true values, e.g. from Building Performance Simulations (BPS), and predicted values (ML estimates)."]},{"cell_type":"code","metadata":{"id":"Ga8peK8mSRLW"},"source":["fig, ax = plt.subplots()\n","# plt.scatter(y_train_pred, y_train, alpha=0.5, s=1, c='g')\n","# plt.scatter(y_train_pred, y_train, alpha=0.3, s=1)\n","plt.scatter(y_train_full, y_train_full_pred, alpha=0.4, s=5, c='g')\n","plt.scatter(y_test, y_test_pred, alpha=0.4, s=5, c='y')\n","\n","plt.xlabel(\"BPS results\")\n","plt.ylabel(\"ML estimates\")\n","\n","lims = [\n","    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n","    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n","]\n","\n","ax.plot(lims, lims, 'r-', alpha=0.5, zorder=0)\n","ax.set_xlim(lims)\n","ax.set_ylim(lims)\n","ax.set_aspect('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XI-dsXIUIbL"},"source":["## New samples"]},{"cell_type":"markdown","metadata":{"id":"QrVP-Re8bQzv"},"source":["Obtain probability densitity functions (PDF) for each column in an input sample matrix, X. Note that it is limited to discrete and continuous distributions."]},{"cell_type":"code","metadata":{"id":"K5hpJ3koUUKX"},"source":["X_pdfs = obtain_pdfs(X, verbose=True)\n","# X_pdfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4JlbhvwBPF6"},"source":["N_new = 10000\n","\n","# Sample N new samples\n","X_new = create_new_samples(X_pdfs, N_new)\n","\n","# Transform machine learning model\n","x_new_prep = full_pipeline.transform(X_new)\n","# x_new_prep = X_new # For Random Forest with no transforms\n","\n","# Make predictions\n","y_new = model.predict(x_new_prep)\n","\n","# Combine input values and predicted output values\n","X_new = pd.DataFrame(X_new, columns=X.columns)\n","Y_new = pd.DataFrame(y_new, columns=y_labels)\n","XY_new = pd.concat([X_new, Y_new], axis=1)\n","\n","XY_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5kdPUOF6A-1"},"source":["See distributions of original and predicted outputs."]},{"cell_type":"code","metadata":{"id":"DkttUn8R3-lq"},"source":["Y_new_ = Y_new.copy()\n","Y_new_.columns = [s + '*' for s in Y_new.columns.to_list()]\n","\n","YY_ = pd.concat([Y, Y_new_], axis=1)\n","YY_.hist(figsize=(Y.shape[1]*3, 5), bins=25, layout=(2, Y.shape[1]), grid=False);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJjDwTMu6LY5"},"source":["Export predicted dataset to csv-file."]},{"cell_type":"code","metadata":{"id":"uOauKJ5LQTN4"},"source":["SAVE_NEW_SAMPLES = True\n","if SAVE_NEW_SAMPLES:\n","  XY_new.to_csv(my_path + file_name_only + '_' + str(N_new) + '.csv', sep='\\t', index=False)"],"execution_count":null,"outputs":[]}]}