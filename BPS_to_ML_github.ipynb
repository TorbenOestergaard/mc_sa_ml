{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BPS_to_ML_github.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MNt1s_qFT3Z8"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"WD3LtyzYtfJk"},"source":["## Option A: Public github notebook"]},{"cell_type":"markdown","metadata":{"id":"iFHv2chhPqdo"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TorbenOestergaard/mc_sa_ml/blob/main/BPS_to_ML_github.ipynb)\n","\n","The fastest way to get started is to work online in the public github notebook.  \n","The content in the github repository 'mc_sa_ml' must be cloned to the current Colab session in order to access the scripts and the example file. \n","\n","- Requirements: No requirements\n","- Changes: Your changes will *not* be saved when closing the browser."]},{"cell_type":"code","metadata":{"id":"KaKR9jg3tfJk"},"source":["!git clone https://github.com/TorbenOestergaard/mc_sa_ml.git\n","\n","my_path = 'mc_sa_ml/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUfQWp0wUHUW"},"source":["## Option B: Google Drive"]},{"cell_type":"markdown","metadata":{"id":"TxDqyNUpOikV"},"source":["You may import the folder Github repository 'mc_sa_ml' and copy it to your Google Drive folder, for example to the 'Colab Notebooks\" subfolder which is created when you make your first Colab notebook.  \n","\n","- Requirements: Google Colab must be allowed to access the content in your Drive folder.\n","- Changes: Your changes will automatically be saved in your local notebook and files will be saved to your Drive folder. "]},{"cell_type":"code","metadata":{"id":"Bx59WND9-TOD"},"source":["# from google.colab import drive \n","# drive.mount('/content/drive')\n","\n","# my_path = '/content/drive/MyDrive/Colab Notebooks/mc_sa_ml/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfnz4e6ty25i"},"source":["## Option C: Local Python interpreter"]},{"cell_type":"markdown","metadata":{"id":"uFGMtYneNtOu"},"source":["Finally, you may clone the 'mc_sa_ml' repository to your local desktop and run it using your own Python interpreter. However, you need to installe the a number of packages (see dependencies below) and set the 'my_path' variable accordingly."]},{"cell_type":"code","metadata":{"id":"Xq2v6k6YOMKA"},"source":["# my_path = 'your_local_path_to_mc_sa_ml_folder'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3x3iX7QULeM"},"source":["## Import dependencies"]},{"cell_type":"code","metadata":{"id":"eb8EuYML-x8R"},"source":["global y_train_pred\n","global y_train_full_pred\n","global y_valid_pred\n","global y_test_pred\n","\n","global x_train_prepared\n","global x_train_full_prepared\n","global x_valid_prepared\n","global x_test_prepared\n","\n","from os import path, makedirs\n","import sys\n","from google.colab import files\n","import io\n","\n","sys.path.append(my_path)\n","\n","import scripts as src\n","from scripts import transform_categorical_features, ordinal_decode_cat_hyperparameters, create_new_samples, print_R2_performance, make_predictions, sa_multiple, obtain_pdfs \n","import SAtom2 as sa\n","\n","# Data analysis and visualization\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import clear_output \n","from ipywidgets import Dropdown\n","from os import path, makedirs\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import r2_score\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPXTHMXy3gWG"},"source":["# Choose datafile"]},{"cell_type":"markdown","metadata":{"id":"_nmr5m9O1Myx"},"source":["## Upload file"]},{"cell_type":"markdown","metadata":{"id":"Ik_Ar4iKTBrG"},"source":["Run cell below to create upload button. Then select a file with Monte Carlo simulations inputs and outputs."]},{"cell_type":"code","metadata":{"id":"Q2oqKygU1O3H"},"source":["uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vipqoyGtTTSQ"},"source":["Run to load the file, if present, transform categorical features to numeric to enable sensitivity analysis and machine learning."]},{"cell_type":"code","metadata":{"id":"CqMxgk7j_Q0k"},"source":["try:\n","  filename_monte_carlo = list(uploaded.keys())[0]\n","except:\n","  filename_monte_carlo = 'test_high_school_i10_o3.tsv'\n","print(f'Filename:   {filename_monte_carlo}')\n","\n","file_extension = filename_monte_carlo.split(\".\")[-1]\n","file_name_only = filename_monte_carlo[:-(len(file_extension)+1)]\n","\n","if file_extension == 'xlsx':\n","  # Excel file. If data in specific sheet, add argument: sheet_name='Sheet1'\n","  XY_raw = pd.read_excel(my_path + filename_monte_carlo, header=0, engine=\"openpyxl\", )\n","elif file_extension == 'csv' or file_extension == 'txt':\n","  XY_raw = pd.read_csv(my_path +  filename_monte_carlo, sep=None)\n","elif file_extension == 'tsv':\n","  XY_raw = pd.read_csv(my_path + filename_monte_carlo, sep='\\t')\n","print(f'N rows:     {XY_raw.shape[0]} \\nN columns:  {XY_raw.shape[1]}')\n","\n","# Transform pcategorical features\n","XY = transform_categorical_features(XY_raw, verbose=False)\n","\n","XY_raw.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8Qxw4OPUqzB"},"source":["Specify the number of inputs and outputs to enable the scripts to distinguish between input columns and output columns."]},{"cell_type":"code","metadata":{"id":"scFUZICa_sHT"},"source":["n_inputs = 10 # Specify the number of input columns\n","n_outputs = 3 # Specify the number of output columns\n","\n","N = XY.shape[0]\n","X = XY.iloc[:N, :n_inputs]\n","Y = XY.iloc[:N, -n_outputs:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAxk2eaMdwSl"},"source":["Optionally, save as txt-file which can be uploaded to e.g. [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/)."]},{"cell_type":"code","metadata":{"id":"d1g57b1Wdvtz"},"source":["print('Saving to: ' + my_path + filename_monte_carlo[:-4] + 'csv')\n","# XY.to_csv(my_path + filename_monte_carlo[:-4] + 'csv', sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJxo73p-Uv0A"},"source":["# Sensitivity Analysis"]},{"cell_type":"markdown","metadata":{"id":"j3xGiM19UC3r"},"source":["Sensitivity analysis is performed using the *TOM* method which estimates the inputs' *total effects* on the outputs. It means that the contribution from 'interaction effects' between e.g. input *X1* and *X2* will be included in the sensitivity measure for both inputs. Thus it can be used in a *factor fixing* context where the object is to identify and potentially fix non-influential inputs (see [Global Sensitivity Analysis: The Primer](http://www.andreasaltelli.eu/file/repository/A_Saltelli_Marco_Ratto_Terry_Andres_Francesca_Campolongo_Jessica_Cariboni_Debora_Gatelli_Michaela_Saisana_Stefano_Tarantola_Global_Sensitivity_Analysis_The_Primer_Wiley_Interscience_2008_.pdf) for sensitivity analysis theory).\n","\n","The *TOM* method may be applied to all outputs simulationeously as means to order the inputs by the overall influence on all outputs. This is makes it easier to observe changes of the overall most important inputs when exploring the data using [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/). It does not simply asign equal weights to all outputs but instead reduce weights to highly correlated outputs. \n","\n","A detailed description of the *TOM* method is available in this [conference paper](https://vbn.aau.dk/da/publications/interactive-building-design-space-exploration-using-regionalized-)."]},{"cell_type":"markdown","metadata":{"id":"fYGqL0z35F2g"},"source":["## Single output"]},{"cell_type":"markdown","metadata":{"id":"oWicxjc1UyPJ"},"source":["Use dropdown to specify which output should be addressed in the sensitivity analysis."]},{"cell_type":"code","metadata":{"id":"vG0TYSv_AGlS"},"source":["output_labels = list(Y.columns)\n","dropdown = Dropdown(options= ['All'] + list(output_labels), description='Output(s):')\n","dropdown"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ff6D0i5yVGiV"},"source":["Set sensitivy arguments and perform analysis.  \n","For large datasets, start with a small J value (e.g. 30) for a single output and see if the SA estimates have converged. If not, increase J until they converge. "]},{"cell_type":"code","metadata":{"id":"Su5uxB7RAM98"},"source":["J = 30              # Number random splits. Increase if values have not converged\n","add_dummy = True    # Specify if a 'dummy' variable should be added\n","\n","if dropdown.value == 'All':\n","  print('Performing sensitivity analysis for all outputs.')\n","  y = Y.copy()\n","else:\n","  print('Performing sensitivity analysis for: ' + dropdown.value)\n","  y = Y.iloc[:, output_labels.index(dropdown.value)]\n","\n","tom = sa.TOM(X, y, J=J, verbose=True, dummy=add_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00Zehmk69jbA"},"source":["`tom.KS_df` contains the J-averaged Kolmogorov-Smirnov distances and can be used to make convergence plots (in relation to J).  \n","In the `df_SA` DataFrame sensitivity measures are rescaled to sum to 100%. Both DataFrames can be used to make your own SA figures."]},{"cell_type":"code","metadata":{"id":"6qb6W2eO-YV1"},"source":["df_SA = pd.DataFrame(columns=['Input', 'SA, tom'])\n","SA_score = tom.KS_df.iloc[-1,:].values # The last J'th averaged sample is used\n","SA_score = np.array([(val / sum(SA_score)) * 100 for val in SA_score])\n","for i, (col, score) in enumerate(zip(tom.KS_df.columns, SA_score)):\n","  df_SA.loc[i] = [col, score]\n","df_SA.sort_values(by=\"SA, tom\", ascending=False) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CM6MzPd2GMcL"},"source":["Optionally, order input columns by their influence with respect to a given output (or all outputs). The most influential will then be placed closest to the output columns, which make analysis in [DataExplorer](https://buildingdesign.moe.dk/dataexplorer/) easier. "]},{"cell_type":"code","metadata":{"id":"n63sTFDOGK1J"},"source":["filename_export = 'sorted_XY'  # Define filename\n","\n","XY_export = XY.copy()\n","sa_sorted_inputs = list(df_SA.drop(df_SA[df_SA['Input'] == 'Dummy'].index).sort_values(by=\"SA, tom\", ascending=True)['Input'])\n","sorted_columns = sa_sorted_inputs + list(output_labels)\n","XY_export = XY_export[sorted_columns]\n","\n","# Save as csv or xlsx\n","XY_export.to_csv(my_path + filename_export + '.csv', sep='\\t', index=False)\n","# XY_export.to_excel(my_path + filename_export + '.xlsx', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9M8qGXR5JZc"},"source":["## All outputs\n","Create a figure with histogram and SA bar plots for all outputs."]},{"cell_type":"code","metadata":{"id":"mK36Vs0RXYnE"},"source":["# Enable figures to be return from function\n","%config InlineBackend.close_figures = False\n","\n","N_samples = X.shape[0] # Optionally, reduce the number of samples for faster SA\n","res, fig = sa_multiple(X.iloc[:N_samples,:], \n","                       Y.iloc[:N_samples,:], \n","                       J=100, \n","                       include_SA_all=False, \n","                       sort_by='all',\n","                       figsize='auto', # (7,5)\n","                       )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZH9gfueo5gPf"},"source":["Optionally, save the figure as jpg, png, or svg."]},{"cell_type":"code","metadata":{"id":"l5LOoj6wIL1I"},"source":["SAVE_FIG = False\n","if SAVE_FIG:\n","  # fig.savefig(my_path + file_name_only + '_TOM_SA.svg')\n","  # fig.savefig(my_path + file_name_only + '_TOM_SA.png')  \n","  fig.savefig(my_path + file_name_only + '_TOM_SA.jpg', dpi=300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxRRaAeqVdrg"},"source":["# Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"bACHMOUaNa8j"},"source":["## Choose output"]},{"cell_type":"markdown","metadata":{"id":"dzsp7pLRYzSw"},"source":["Run below cell the make a dropdown to choose which output should be used in the supervised machine learning. Alternatively, all outputs can be included but accuracy will be lower for the individual outputs."]},{"cell_type":"code","metadata":{"id":"2FMUQUMzNC7q"},"source":["output_labels = list(Y.columns)\n","dropdown_ml = Dropdown(options=['All (NN only)'] + list(output_labels), description='Output:')\n","dropdown_ml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52iMYedAVf0i"},"source":["## Prepare data"]},{"cell_type":"markdown","metadata":{"id":"G66wp8kEVmV6"},"source":["Split dataset into training set, validation set, and test set."]},{"cell_type":"code","metadata":{"id":"qoBUUpynKl0v"},"source":["VALID_SIZE = 0.25\n","TEST_SIZE = 0.25\n","shuffle_state = True\n","np.random.seed(42)\n","\n","# Create a test set and a \"full\" training set to be split into training and validation sets\n","train_set_full, test_set = train_test_split(XY, test_size=TEST_SIZE, shuffle=shuffle_state)\n","train_set, valid_set = train_test_split(train_set_full, test_size=(VALID_SIZE / (1 - TEST_SIZE)), shuffle=shuffle_state)\n","\n","total_sim = train_set.shape[0] + valid_set.shape[0] + test_set.shape[0]\n","display(f'Total of {total_sim} simulations. Length of data_raw: {len(XY)}.')\n","display(f'train set {train_set.shape}, valid set{valid_set.shape}, test set {test_set.shape}.')\n","display(f'Split ratios: {round((train_set.shape[0]/total_sim*100))} / {round((valid_set.shape[0]/total_sim*100))} / {round((test_set.shape[0]/total_sim*100))} %')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42qLfXukVwFh"},"source":["Split each set into input and output subsets and transform data for machine learning."]},{"cell_type":"code","metadata":{"id":"l-SxqgadL0iK"},"source":["x_train_full = train_set_full.iloc[:, :n_inputs]\n","x_train = train_set.iloc[:, :n_inputs]\n","x_valid = valid_set.iloc[:, :n_inputs]\n","x_test = test_set.iloc[:, :n_inputs]\n","\n","if dropdown_ml.value[:3] == 'All':\n","  y_train_full = train_set_full.iloc[:, n_inputs:]\n","  y_train = train_set.iloc[:, n_inputs:]\n","  y_valid = valid_set.iloc[:, n_inputs:]\n","  y_test = test_set.iloc[:, n_inputs:]\n","  y_labels = train_set.iloc[:, n_inputs:].columns.to_list()\n","else:\n","  y_train_full = train_set_full[dropdown_ml.value].copy()\n","  y_train = train_set[dropdown_ml.value].copy()\n","  y_valid = valid_set[dropdown_ml.value].copy()\n","  y_test = test_set[dropdown_ml.value].copy()\n","  y_labels = [dropdown_ml.value]\n","\n","num_pipeline = Pipeline([('standard_scaler', StandardScaler()),])\n","numerical_attr = x_train.select_dtypes(include=['float64', 'int64']).columns\n","\n","full_pipeline = ColumnTransformer([\n","    (\"num\", num_pipeline, numerical_attr), \n","    # (\"ord\", ordinal_pipeline, ord_attr),\n","    # (\"cat\", cat_pipeline, categorical_attr),\n","])\n","\n","x_train_prepared = full_pipeline.fit_transform(x_train)\n","x_train_full_prepared = full_pipeline.fit_transform(x_train_full)\n","x_valid_prepared = full_pipeline.transform(x_valid)\n","x_test_prepared = full_pipeline.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2B3Z5E3aV3Lm"},"source":["## Train models"]},{"cell_type":"markdown","metadata":{"id":"qgN0QizrWWun"},"source":["### Neural network"]},{"cell_type":"markdown","metadata":{"id":"lSTLgvyzJREA"},"source":["#### Single model"]},{"cell_type":"markdown","metadata":{"id":"9NA-MMEWEBNm"},"source":["Train a neural network *without* hyperparameter optimization. "]},{"cell_type":"code","metadata":{"id":"0IvS-BCdTomA"},"source":["# Define model parameters\n","model = MLPRegressor(hidden_layer_sizes=(20, 20), \n","                  learning_rate_init=0.05, \n","                  alpha=0.001, \n","                  early_stopping=True, \n","                  max_iter=500,  )\n","\n","# Train neural network\n","model.fit(x_train_full_prepared, y_train_full.values)\n","\n","# Make predictions\n","# (y_train_pred, y_train_full_pred, y_valid_pred, y_test_pred) = make_predictions(model);\n","y_train_pred = model.predict(x_train_prepared)\n","y_train_full_pred = model.predict(x_train_full_prepared)\n","y_valid_pred = model.predict(x_valid_prepared)\n","y_test_pred = model.predict(x_test_prepared)\n","\n","# Clear output cell and show only R² values (averaged if multiple outputs)\n","clear_output() \n","print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSQWCyuaEQyy"},"source":["See R²-values for each output."]},{"cell_type":"code","metadata":{"id":"V4HVq7rUCZTJ"},"source":["print(f'{\" \" * 18}TRAIN / TEST')\n","if y_train.ndim == 1:\n","  # print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f} -> Output: \"{y_train.to_frame().columns[0]}\"')\n","  print(f'{y_train.to_frame().columns[0]:>15}   {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}')\n","else:\n","  for i in range(y_train_pred.shape[1]):\n","    weights = np.zeros(y_train_pred.shape[1])\n","    weights[i] = 1\n","    r2_train_i = r2_score(y_train_full, y_train_full_pred, multioutput=weights)\n","    r2_test_i = r2_score(y_test, y_test_pred, multioutput=weights)\n","    print(f'{y_train.columns[i]:>15}   {r2_train_i:.3f} / {r2_test_i:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFTOMFWFJbQW"},"source":["#### Grid-search optimization"]},{"cell_type":"markdown","metadata":{"id":"5GLdkCgaEVqW"},"source":["Perform grid search to optimize hyperparameters. For more hyperparameters see [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).  \n","The number trained models equals the product of options for each hyperparameter multiplied by the number of cross validations (cv)."]},{"cell_type":"code","metadata":{"id":"eHyqvHwYblJt"},"source":["cross_validations = 3\n","\n","# Define hyperparameters to run grid search over\n","param_grid = [\n","              {'hidden_layer_sizes':[(20, 20), (50, 50), (20, 20, 20)], \n","               'learning_rate_init':[0.001, 0.01, 0.03], \n","              #  'learning_rate': ['constant', 'invscaling', 'adaptive'],\n","               'alpha':[0.001, 0.01], # default 0.001\n","               }\n","              ]\n","\n","# Setup fixed hyperparameters\n","model = MLPRegressor(#alpha=0.001, # default 0.0001, \n","                  early_stopping=True, max_iter=500, )\n","\n","# Set grid search options\n","grid_search = GridSearchCV(model, param_grid, cv=cross_validations,\n","                          scoring='neg_mean_squared_error',\n","                          return_train_score=True, verbose=1, n_jobs=-1)\n","\n","# Run grid search\n","grid_search.fit(x_train_full_prepared, y_train_full.values)\n","\n","clear_output() # Clear output cell\n","\n","# See cross validation results (RMSE) from grid search\n","cvres = grid_search.cv_results_ # Cross validation results from grid search\n","params_count = len(cvres['params'][0].keys()) # Number of hyperparameters\n","params_labels = list(cvres['params'][0].keys()) # Labels for hyperparameters\n","cvres_df = pd.DataFrame(columns=['RMSE'] + list(cvres['params'][0].keys()))\n","for i in range(len(cvres['mean_test_score'])):\n","  cvres_df = cvres_df.append({'RMSE': np.sqrt(-cvres['mean_test_score'][i]), **cvres['params'][i]}, ignore_index=True, )\n","\n","# Performance of best model\n","model = grid_search.best_estimator_\n","# (y_train_pred, y_train_full_pred, y_valid_pred, y_test_pred) = make_predictions(model);\n","y_train_pred = model.predict(x_train_prepared)\n","y_train_full_pred = model.predict(x_train_full_prepared)\n","y_valid_pred = model.predict(x_valid_prepared)\n","y_test_pred = model.predict(x_test_prepared)\n","print(f'Best model:  {grid_search.best_params_} \\nLowest RMSE: {cvres_df.RMSE.min():.3f}')\n","print(f'R² train/test: {r2_score(y_train_full, y_train_full_pred):.3f} / {r2_score(y_test, y_test_pred):.3f}\\n')\n","\n","display(cvres_df.sort_values(by='RMSE'))\n","cvres_df = ordinal_decode_cat_hyperparameters(cvres_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGSUfO4ROnl3"},"source":["Perform SA on hyperparameters to see which have most influence on performance. This indicates which hyperparameters that require most attention and could be discretized further in additional grid searches. May not work if only two parameters have been varied with two discretizations."]},{"cell_type":"code","metadata":{"id":"PNNy6YiSr3fp"},"source":["sa.TOM(cvres_df.drop('RMSE', axis=1), cvres_df.RMSE, J=100, dummy=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"elJSZ17VOvgL"},"source":["See hyperparameter distributions leading to the 20% best RMSE.  \n","This may reveal suitable values for the most important hyperparameters (among the considered parameters)."]},{"cell_type":"code","metadata":{"id":"4ZegBcNcODd0"},"source":["cvres_df.loc[cvres_df.RMSE > cvres_df.RMSE.quantile(0.8)].drop('RMSE', axis=1).hist(\n","    bins=100, figsize=(cvres_df.shape[1]*3,2.5), layout=(1, cvres_df.shape[1]-1), grid=False);\n","cvres_df.loc[cvres_df.RMSE > cvres_df.RMSE.quantile(0.8)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1nmRRmfWQIA"},"source":["### Plot y-y scatter"]},{"cell_type":"markdown","metadata":{"id":"jh8z2Bfla_bw"},"source":["Create a plot of true values, e.g. from Building Performance Simulations (BPS), and predicted values (ML estimates)."]},{"cell_type":"code","metadata":{"id":"Ga8peK8mSRLW"},"source":["fig, ax = plt.subplots()\n","# plt.scatter(y_train_pred, y_train, alpha=0.5, s=1, c='g')\n","# plt.scatter(y_train_pred, y_train, alpha=0.3, s=1)\n","plt.scatter(y_train_full, y_train_full_pred, alpha=0.4, s=5, c='g')\n","plt.scatter(y_test, y_test_pred, alpha=0.4, s=5, c='y')\n","\n","plt.xlabel(\"BPS results\")\n","plt.ylabel(\"ML estimates\")\n","\n","lims = [\n","    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n","    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n","]\n","\n","ax.plot(lims, lims, 'r-', alpha=0.5, zorder=0)\n","ax.set_xlim(lims)\n","ax.set_ylim(lims)\n","ax.set_aspect('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XI-dsXIUIbL"},"source":["## New samples"]},{"cell_type":"markdown","metadata":{"id":"QrVP-Re8bQzv"},"source":["Obtain probability densitity functions (PDF) for each column in an input sample matrix, X. Note that it is limited to discrete and continuous distributions."]},{"cell_type":"code","metadata":{"id":"K5hpJ3koUUKX"},"source":["X_pdfs = obtain_pdfs(X, verbose=True)\n","# X_pdfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4JlbhvwBPF6"},"source":["N_new = 10000\n","\n","# Sample N new samples\n","X_new = create_new_samples(X_pdfs, N_new)\n","\n","# Transform machine learning model\n","x_new_prep = full_pipeline.transform(X_new)\n","# x_new_prep = X_new # For Random Forest with no transforms\n","\n","# Make predictions\n","y_new = model.predict(x_new_prep)\n","\n","# Combine input values and predicted output values\n","X_new = pd.DataFrame(X_new, columns=X.columns)\n","Y_new = pd.DataFrame(y_new, columns=y_labels)\n","XY_new = pd.concat([X_new, Y_new], axis=1)\n","\n","XY_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5kdPUOF6A-1"},"source":["See distributions of original and predicted outputs."]},{"cell_type":"code","metadata":{"id":"DkttUn8R3-lq"},"source":["Y_new_ = Y_new.copy()\n","Y_new_.columns = [s + '*' for s in Y_new.columns.to_list()]\n","\n","YY_ = pd.concat([Y, Y_new_], axis=1)\n","YY_.hist(figsize=(Y.shape[1]*3, 5), bins=25, layout=(2, Y.shape[1]), grid=False);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJjDwTMu6LY5"},"source":["Export predicted dataset to csv-file."]},{"cell_type":"code","metadata":{"id":"uOauKJ5LQTN4"},"source":["SAVE_NEW_SAMPLES = True\n","if SAVE_NEW_SAMPLES:\n","  XY_new.to_csv(my_path + file_name_only + '_' + str(N_new) + '.csv', sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpVnmKpJOJtf"},"source":["# Dev"]},{"cell_type":"markdown","metadata":{"id":"yw5F75pN8yTS"},"source":["## Links\n","\n","- [Allow figures to be returned from functions](https://stackoverflow.com/questions/17549713/how-to-have-a-function-return-a-figure-in-python-using-matplotlib\n",")\n","- [Number of figures](https://stackoverflow.com/questions/3410976/how-to-round-a-number-to-significant-figures-in-python)\n","- [Nested dicts](https://www.learnbyexample.org/python-nested-dictionary/)\n","- [Merge dictionaries](https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression-taking-union-of-dictiona)\n","- Max letters in list of strings: `max(a_list, key=len)`\n","- [Clear output using code](https://stackoverflow.com/questions/24816237/ipython-notebook-clear-cell-output-in-code)\n"]},{"cell_type":"markdown","metadata":{"id":"Vki1ikFTWFyi"},"source":["## Helper functions"]},{"cell_type":"code","metadata":{"id":"Lhw1dSIVDwms"},"source":["def transform_categorical_features(XY_raw, verbose=True):\n","  \"\"\"\n","  Transforms categorical features to numerical using ordinal encoder.\n","      \n","  Parameters\n","  ----------\n","      XY_raw : DataFrame, input/output matrix\n","\n","  Returnts\n","  ----------\n","      XY : DataFrame, transformed i/o matrix with numerical features\n","    \n","  \"\"\"  \n","\n","  # Obtain labels for categorical features (if present)\n","  ord_attr = XY_raw.select_dtypes(exclude=['float64', 'int64']).columns\n","\n","  if len(ord_attr) == 0:\n","    print(f'No categorical features detected.')\n","    XY = XY_raw.copy()\n","\n","  else:\n","    print(f'Found categorical features: {ord_attr}')\n","\n","    # Pipeline for ordinal encoder\n","    ordinal_pipeline = Pipeline([\n","                                # ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n","                                ('encode', OrdinalEncoder()),\n","                                ])\n","\n","    # Full pipeline, transform categorical features and update dataframe\n","    full_pipeline = ColumnTransformer([(\"ord\", ordinal_pipeline, ord_attr),],\n","                                      remainder='passthrough')\n","    XY_arr = full_pipeline.fit_transform(XY_raw)\n","\n","    XY = pd.DataFrame(XY_arr, columns=ord_attr.to_list() + XY_raw.columns.drop(ord_attr).to_list())\n","    if verbose:\n","      display(XY.head())\n","    \n","  return XY"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZKaAICD9WYg"},"source":["def create_new_samples(X_pdfs, N):\n","  \"\"\"\n","  Sample N new inputs from probability densitity distributions (PDF).\n","  Distributions are limited to uniform, continuous and uniform, discrete.\n","      \n","  Parameters\n","  ----------\n","      X_pdfs : dictionary, decription of PDF for each variable input in X\n","      N : int, number of new samples\n","  \"\"\"  \n","  # Initialize array for new samples and list for inputs labels\n","  x_arr = np.zeros([N, len(X_pdfs)])\n","  x_labels = []\n","\n","  # Loop through dictionary - each describing variable input, Xi\n","  for i, (xvar, nest_dict) in enumerate(X_pdfs.items()):\n","    x_labels = x_labels + [nest_dict['param']]\n","\n","    if nest_dict['pdf'] == 'uniform':\n","      x_temp = np.random.uniform(nest_dict['args'][0], nest_dict['args'][1], N)\n","    elif nest_dict['pdf'] == 'discrete':\n","      x_temp = np.random.choice(nest_dict['args'], N)\n","\n","    x_arr[:,i] = np.reshape(x_temp,-1)\n","  \n","  X_new = pd.DataFrame(x_arr, columns=x_labels)\n","\n","  return X_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9Xh_Ja8WH1w"},"source":["def print_R2_performance(model, test=False):\n","    \"\"\"\n","    Print R2 score for training and validation set\n","    \n","    Requires: x_train_prepared, x_valid_prepared, y_train, y_valid\n","    \n","    Parameters\n","    ----------\n","        model : scikit-learn model\n","        test : boolean, True to include performance on test set\n","    \"\"\"\n","    y_train_pred = model.predict(x_train_full.values)\n","    # y_valid_pred = model.predict(x_valid_prepared)\n","    r2_train = r2_score(y_train_full, y_train_pred)\n","    # r2_valid = r2_score(y_valid, y_valid_pred)\n","#     message = f'R²(train) = {r2_train:.3f} \\t R²(valid) = {r2_valid:.3f}'\n","    message = f'# {r2_train:.3f}'#' / {r2_valid:.3f}'\n","    \n","    if test:\n","        y_test_pred = model.predict(x_test.values)\n","        r2_test = r2_score(y_test, y_test_pred)\n","        message = message + f' / {r2_test:.3f}'\n","    print(message)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-vRVCYLbKy2"},"source":["def make_predictions(model):\n","  y_train_pred = model.predict(x_train_prepared)\n","  y_train_full_pred = model.predict(x_train_full_prepared)\n","  y_valid_pred = model.predict(x_valid_prepared)\n","  y_test_pred = model.predict(x_test_prepared)\n","  return y_train_pred, y_train_full_pred, y_valid_pred, y_test_pred\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNUNKftvUtZA"},"source":["def sa_multiple(X, Y, J=50, include_SA_all=False, sort_by=None, figsize='auto'):\n","  \"\"\"Perform sensitivity analysis for each output in matrix using SAtom\n","\n","  Parameters:\n","  -----------\n","  X (pandas.DataFrame): DataFrame with numeric input values\n","  Y (pandas.DataFrame): DataFrame with numeric output values\n","  J : int, default=50\n","    Number of repeated randomly selected samples. \n","  include_SA_all : {True, False}, default=False\n","    If True, TOM SA will also be performed for all outputs\n","  sort_by : {None, 'alphabetically', 'all'} or output label\n","    Decide how to sort the input labels on bar plots, e.g. using SA values.  \n","  figsize : {'auto', (float, float)} figure width and height in inches. 'auto'\n","    will set the figure size automatically from the number of inputs and outputs  \n","\n","  Returns:\n","  --------\n","  Y_SA (pandas.DataFrame): indexed with input names and columns with SA metrics\n","    for each output\n","  fig (matplotlib.Figure): plot figure\n","\n","  References\n","  ----------\n","  Østergård, T., Jensen, R.L., and Maagaard, S.E. (2017)\n","      Interactive Building Design Space Exploration Using Regionalized \n","      Sensitivity Analysis, 15th conference of the International Building \n","      Performance Simulation Association, 7-9 August 2017, San Francisco, USA  \n","\n","  \"\"\"\n","  \n","  n_outputs = Y.shape[1] + 1 if include_SA_all else Y.shape[1]\n","  n_inputs = Y.shape[1]\n","  Y_SA = np.zeros([X.shape[1], Y.shape[1]])\n","\n","  # Loop and perform SA for each output\n","  for i, col in enumerate(Y.columns):\n","    print(f'({i+1}/{n_outputs}) {\"Performs TOM SA for:\" :>20}  {col}')\n","    tom = sa.TOM(X, Y[col], J=J, verbose=False, dummy=False)\n","    KS_means = np.reshape(np.transpose(tom.KS_df.tail(1).values),-1)\n","    KS_means = KS_means / sum(KS_means) * 100 # Convert to percentages\n","    Y_SA[:, i] = KS_means\n","\n","  # Convert array to DataFrame\n","  Y_SA = pd.DataFrame(data=Y_SA, index=list(X.columns.values), columns=Y.columns)\n","  \n","  if include_SA_all:\n","    print(f'({n_outputs}/{n_outputs}) {\"Performs TOM SA for:\" :>20}  All outputs')\n","    tom = sa.TOM(X, Y, J=J*2, verbose=False, dummy=False)\n","    KS_means = np.reshape(np.transpose(tom.KS_df.tail(1).values),-1)\n","    KS_means = KS_means / sum(KS_means) * 100 # Convert to percentages    \n","    Y_SA.insert (0, \"All\", KS_means)\n","\n","  # Sort inputs by output(s)\n","  if sort_by != None:\n","    if sort_by == 'alphabetically':\n","      Y_SA.sort_index(axis=0, inplace=True)\n","    if sort_by == 'all':\n","      if include_SA_all == True:\n","        Y_SA = Y_SA.sort_values(by='All', ascending=False) \n","    if sort_by in Y_SA.columns:\n","      Y_SA = Y_SA.sort_values(by=sort_by, ascending=False) \n","\n","  # Plot distributions and horisontal barplots with SA results\n","  print('Preparing plots')\n","  \n","  if figsize == 'auto': figsize = (2+n_outputs*1.7, 2+n_inputs*.3)\n","  fig, axes = plt.subplots(2, n_outputs, \n","                           figsize=figsize,  # (2+n_outputs*1.7, 1+n_inputs*1)\n","                           constrained_layout=True, gridspec_kw={'height_ratios': [1, 3]},\n","                           ) #  sharey='row',\n","\n","  for i_plot in range(n_outputs):\n","    # Create histograms (except for \"all outputs\")\n","    if i_plot == 0 and include_SA_all == True:\n","      axes[0,0].axis('off')\n","    else:\n","      sns.histplot(ax=axes[0, i_plot], data=Y, x=Y_SA.columns[i_plot], \n","                 stat=\"probability\", element=\"step\", fill=True, bins=25)\n","    \n","    # Create SA bar plots            \n","    sns.barplot(ax=axes[1, i_plot], y=Y_SA.index, x=Y_SA.iloc[:,i_plot])\n","    axes[1, i_plot].spines['top'].set_visible(False)  \n","    axes[1, i_plot].spines['right'].set_visible(False)  \n","\n","    # Add labels\n","    for i, val in enumerate(Y_SA.iloc[:,i_plot].values):\n","        axes[1, i_plot].text(val+0.1, i+0.2, str(round(val,1)), size=8)\n","\n","    axes[0, i_plot].set_title(Y_SA.columns[i_plot], fontsize=10)\n","    axes[0, i_plot].xaxis.label.set_visible(False)\n","    if i_plot >= 1:\n","      axes[0, i_plot].get_yaxis().set_visible(False)\n","      axes[1, i_plot].get_yaxis().set_visible(False)\n","    else:\n","      axes[0, i_plot].yaxis.set_ticks([])\n","\n","  return Y_SA, fig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ko-cmlFQ9DrF"},"source":["import math\n","round_to_n = lambda x, n: x if x == 0 else round(x, -int(math.floor(math.log10(abs(x)))) + (n - 1))\n","\n","def obtain_pdfs(X, n_significant_figures=3, verbose=True):\n","  \"\"\"\n","  Obtain pdfs for each column in an input sample matrix, X.\n","  Assumes discrete distribution if number of unique values are less than 20, \n","  otherwise a continuous, uniform distribution \n","  \n","  Parameters\n","  ----------\n","    X {DataFrame} : Monte Carlo input values\n","    n_significat_figures : integer to round min/max values for uniform PDF\n","\n","  Returns:\n","  --------\n","    X_pdfs {dict} : nested dict for each column in X\n","      e.g. 'x1' : {'idx':0, 'param': 'myvar', 'pdf': 'uniform', 'args': [0 2]}\n","  \"\"\"\n","  X_pdfs = {}\n","  n_max_letters = len(max(X.columns, key=len)) + 2 # For optimal space in print\n","  for i, col in enumerate(X.columns):\n","    if len(X[col].unique()) <= 20:\n","      pdf = 'discrete'\n","      args = X[col].unique()\n","    else:\n","      pdf = 'uniform'      \n","      args = [round_to_n(X[col].min(), n_significant_figures), \n","              round_to_n(X[col].max(), n_significant_figures)]\n","    \n","    X_pdfs['x'+str(i)] = {'idx':i, 'param':col, 'pdf':pdf, 'args':args}    \n","    \n","    if verbose:\n","      print(f'x{str(i):<2} {col:<{n_max_letters}} {pdf:<10} {args}')\n","\n","  return X_pdfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVy6xQdOeW54"},"source":["# # Debugging ordinal_decode_cat_hyperparameters\n","# cvres_df = pd.DataFrame(columns=['RMSE'] + list(cvres['params'][0].keys()))\n","# for i in range(len(cvres['mean_test_score'])):\n","#   cvres_df = cvres_df.append({'RMSE': np.sqrt(-cvres['mean_test_score'][i]), **cvres['params'][i]}, ignore_index=True, )\n","\n","# # cvres_df.loc[cvres_df['hidden_layer_sizes'] == (20, 20), 'hidden_layer_sizes'] = 'str'\n","# cvres_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"maOdXD3vKxGa"},"source":["def ordinal_decode_cat_hyperparameters_debug(cvres_df):\n","  \"\"\"\n","  Ordinal decode categorical columns in grid search results' dataframe.\n","  Afterwards, sensitivity analysis can be performed to see which hyperparameters\n","  seems to affect the RMSE performance the most.\n","  \n","  Parameters\n","  ----------\n","    cvres_df {DataFrame} : Grid-search values and corresponding RMSE score\n","\n","  Returns:\n","  --------\n","    cvres_df\n","  \"\"\"\n","  numerical_attr = cvres_df.select_dtypes(include=['float64', 'int64']).columns\n","  ord_attr = cvres_df.select_dtypes(exclude=['float64', 'int64']).columns\n","\n","  # If ordinal column contains tuple, tuple values are first transformed to  \n","  # unique strings. Otherwise, small_pipeline sometimes fails.\n","  for col in ord_attr:\n","    if type(cvres_df[col].unique()[0]) == tuple:\n","      for tup in cvres_df[col].unique():\n","        cum_str = ''\n","        for val in tup:\n","          cum_str = cum_str + '_' + str(val)\n","        cvres_df.loc[cvres_df[col] == tup, col] = cum_str\n","\n","  ordinal_pipeline = Pipeline([('encode', OrdinalEncoder())])\n","  small_pipeline = ColumnTransformer([(\"ord\", ordinal_pipeline, ord_attr)], \n","                                    remainder='passthrough')\n","  \n","  cvres_df = small_pipeline.fit_transform(cvres_df)\n","\n","  # Convert transformed array to frame and reinsert RMSE as first column\n","  cvres_df = pd.DataFrame(cvres_df, columns=list(ord_attr) + list(numerical_attr))\n","  rmse_column = cvres_df.pop('RMSE')\n","  cvres_df.insert(0, 'RMSE', rmse_column)\n","\n","  return cvres_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcbtfI0xV5oz"},"source":["## Random Forest"]},{"cell_type":"code","metadata":{"id":"YQZgUJmrLbfY"},"source":["param_grid = [{'max_depth':[8, 10, 12, ], 'max_features': [2, 3, 4], }]\n","\n","forest_reg = RandomForestRegressor(n_jobs=-1, verbose=0, bootstrap=False, n_estimators=100,)\n","\n","grid_search = GridSearchCV(forest_reg, param_grid, cv=3,\n","                          scoring='neg_mean_squared_error',\n","                          return_train_score=True)\n","\n","grid_search.fit(x_train_full.values, y_train_full.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRbgbyJCPeNL"},"source":["cvres = grid_search.cv_results_\n","for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n","    print(np.sqrt(-mean_score), params)\n","\n","print(f'\\n Best parameters: {grid_search.best_params_} \\n R² for validation / test set: ')\n","rf_best = grid_search.best_estimator_\n","print_R2_performance(rf_best, test=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61VMzjVD3qbk"},"source":["params_count = len(cvres['params'][0].keys()) # Number of hyperparameters\n","params_labels = list(cvres['params'][0].keys()) # Labels for hyperparameters\n","cvres_df = pd.DataFrame(columns=['score'] + list(cvres['params'][0].keys()))\n","for i in range(len(cvres['mean_test_score'])):\n","  cvres_df = cvres_df.append({'score': cvres['mean_test_score'][i], **cvres['params'][i]}, ignore_index=True, )\n","\n","cvres_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPwS1dwOonNP"},"source":["Make predictions for all data sets."]},{"cell_type":"code","metadata":{"id":"jW5Id8c4SE7Y"},"source":["y_train_pred = rf_best.predict(x_train.values)\n","y_train_full_pred = rf_best.predict(x_train_full.values)\n","y_valid_pred = rf_best.predict(x_valid.values)\n","y_test_pred = rf_best.predict(x_test.values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIKhkK3686ee"},"source":["## Code snippets"]},{"cell_type":"code","metadata":{"id":"iJntpgcop0E7"},"source":["# forest = RandomForestRegressor(n_estimators=100, max_features=2, max_depth=3, bootstrap=True, n_jobs=-1, verbose=0, criterion='mse') #max_leaf_nodes=200, \n","# forest.fit(cvres_df.drop('RMSE', axis=1).values, cvres_df.RMSE.values)\n","# forest.feature_importances_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vw6A17Sll826"},"source":["# import plotly.express as px\n","# px.parallel_coordinates(cvres_df, color=\"RMSE\", height=300, width=500,\n","#                         color_continuous_scale=px.colors.diverging.Tealrose)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKHfEfEunR3E"},"source":["cvres_df.loc[cvres_df['hidden_layer_sizes'] == (20,20), 'hidden_layer_sizes'] = 220\n","cvres_df.loc[cvres_df['hidden_layer_sizes'] == (50,50), 'hidden_layer_sizes'] = 250\n","cvres_df.loc[cvres_df['hidden_layer_sizes'] == (20,20,20), 'hidden_layer_sizes'] = 320\n","cvres_df['hidden_layer_sizes'] = cvres_df['hidden_layer_sizes'].astype('float64')\n","cvres_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXhiaIiCUgHI"},"source":["(Vincent dataset) Split SHGC and LT and keep only one input with numeric values"]},{"cell_type":"code","metadata":{"id":"7QzmkJhKA0cy"},"source":["# X[['SF','LT']] = X['in:SF LT'].str.split('/',expand=True)\n","# X.drop(columns=['in:SF LT', 'LT'], inplace=True)\n","# X.SF = X.SF.astype('float64')\n","# XY = pd.merge(X, Y, left_index=True, right_index=True, how='outer')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-JTGr87HSe3"},"source":["x = XY[['insulation_type', 'blind_position', 'light_control', 'hx_system']].copy()\n","x1 = XY['insulation_type'].copy()\n","# x['blind_position'].value_counts()\n","x1.value_counts()\n","enc = OrdinalEncoder()\n","enc.fit_transform(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K40sOgIn0wjt"},"source":["forest = RandomForestRegressor(n_estimators=10, max_features=5, max_depth=5, bootstrap=True, n_jobs=-1, verbose=0, criterion='mse') #max_leaf_nodes=200, \n","forest.fit(x_train_prepared, y_train)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvcSb2LeRYGt"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def sa_multiple(X, Y, J=50, plot=True, verbose=True):\n","  \"\"\"Perform sensitivity analysis for each output in matrix using SAtom\n","\n","  Parameters:\n","  X (pandas.DataFrame): DataFrame with numeric input values\n","  Y (pandas.DataFrame): DataFrame with numeric output values\n","\n","  Returns:\n","  pandas.DataFrame indexed with input names and columns with SA metrics for each output\n","\n","  \"\"\"\n","  # Loop and perform SA for each output\n","  Y_SA = np.zeros([X.shape[1], Y.shape[1]])\n","  for i, col in enumerate(Y.columns):\n","    print(f'({i+1}/{Y.shape[1]}) {\"Performs TOM SA for:\" :>20}  {col}')\n","    tom = sa.TOM(X, Y[col], J=J, verbose=False, dummy=False)\n","    KS_means = np.reshape(np.transpose(tom.KS_df.tail(1).values),-1)\n","    KS_means = KS_means / sum(KS_means) * 100 # Convert to percentages\n","    Y_SA[:, i] = KS_means\n","    \n","  # Convert SA \n","  Y_SA = pd.DataFrame(data=Y_SA, index=list(X.columns.values), columns=Y.columns)\n","\n","  # Plot distributions and horisontal barplots with SA results\n","  if plot:\n","    n_outputs = Y_SA.shape[1]\n","    n_inputs = Y_SA.shape[0]\n","    fig, axes = plt.subplots(2, n_outputs, figsize=(2+n_outputs*1.7, 1+n_inputs*0.3), constrained_layout=True, gridspec_kw={'height_ratios': [1, 3]}) #  sharey='row',\n","    # fig.suptitle('Følsomhedsanalyse');\n","    for i_plot in range(n_outputs):\n","      sns.histplot(ax=axes[0, i_plot], data=Y, x=Y.columns[i_plot], stat=\"probability\", element=\"step\", fill=True, bins=25)\n","      sns.barplot(ax=axes[1, i_plot], y=Y_SA.index, x=Y_SA.iloc[:,i_plot])\n","\n","      # Add labels\n","      axes[1, i_plot].spines['top'].set_visible(False)  \n","      axes[1, i_plot].spines['right'].set_visible(False)  \n","      for i, val in enumerate(Y_SA.iloc[:,i_plot].values):\n","          axes[1, i_plot].text(val+0.1, i+0.2, str(round(val,1)), size=8)\n","\n","      axes[0, i_plot].set_title(Y_SA.columns[i_plot], fontsize=10)\n","      axes[0, i_plot].xaxis.label.set_visible(False)\n","      if i_plot >= 1:\n","        axes[0, i_plot].get_yaxis().set_visible(False)\n","        axes[1, i_plot].get_yaxis().set_visible(False)\n","      else:\n","        axes[0, i_plot].yaxis.set_ticks([])\n","\n","  return Y_SA, fig  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYPfgqDQdbCk"},"source":["for id, info in res.items():\n","    print(\"\\nVar:\", id)\n","    for key in info:\n","        print(key + ':', info[key])"],"execution_count":null,"outputs":[]}]}